{
  "3": {
    "inputs": {
      "seed": 531047640091374,
      "steps": 4,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 0.7,
      "model": [
        "75",
        0
      ],
      "positive": [
        "76",
        0
      ],
      "negative": [
        "77",
        0
      ],
      "latent_image": [
        "88",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "8": {
    "inputs": {
      "samples": [
        "3",
        0
      ],
      "vae": [
        "39",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "39": {
    "inputs": {
      "vae_name": "qwen_image_vae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "60": {
    "inputs": {
      "filename_prefix": "ComfyUI-Restored",
      "images": [
        "8",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "66": {
    "inputs": {
      "shift": 3,
      "model": [
        "89",
        0
      ]
    },
    "class_type": "ModelSamplingAuraFlow",
    "_meta": {
      "title": "ModelSamplingAuraFlow"
    }
  },
  "75": {
    "inputs": {
      "strength": 1,
      "model": [
        "66",
        0
      ]
    },
    "class_type": "CFGNorm",
    "_meta": {
      "title": "CFGNorm"
    }
  },
  "76": {
    "inputs": {
      "prompt": "Restore this old photo: remove scratches, dust spots, reflections, and noise; repair tears, folds, and damaged areas; correct fading and color drift; sharpen faces and fabric texture. Fill in any missing or cropped parts of the photo realistically, matching the original style, texture, and lighting. Preserve original faces, pose, clothing, and background without changing composition. Do not add new objects or distortions. Please also colourize the photo.",
      "clip": [
        "102",
        0
      ],
      "vae": [
        "39",
        0
      ],
      "image": [
        "93",
        0
      ]
    },
    "class_type": "TextEncodeQwenImageEdit",
    "_meta": {
      "title": "TextEncodeQwenImageEdit"
    }
  },
  "77": {
    "inputs": {
      "prompt": "",
      "clip": [
        "102",
        0
      ],
      "vae": [
        "39",
        0
      ],
      "image": [
        "93",
        0
      ]
    },
    "class_type": "TextEncodeQwenImageEdit",
    "_meta": {
      "title": "TextEncodeQwenImageEdit"
    }
  },
  "78": {
    "inputs": {
      "image": "old_photo_example.webp"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "88": {
    "inputs": {
      "pixels": [
        "93",
        0
      ],
      "vae": [
        "39",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "89": {
    "inputs": {
      "lora_name": "Qwen-Image-Lightning-4steps-V1.0.safetensors",
      "strength_model": 1,
      "model": [
        "101",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoraLoaderModelOnly"
    }
  },
  "93": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "megapixels": 1,
      "image": [
        "78",
        0
      ]
    },
    "class_type": "ImageScaleToTotalPixels",
    "_meta": {
      "title": "Scale Image to Total Pixels"
    }
  },
  "101": {
    "inputs": {
      "unet_name": "Qwen_Image_Edit-Q6_K.gguf"
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "Unet Loader (GGUF)"
    }
  },
  "102": {
    "inputs": {
      "clip_name": "Qwen2.5-VL-7B-Instruct-Q6_K.gguf",
      "type": "qwen_image"
    },
    "class_type": "CLIPLoaderGGUF",
    "_meta": {
      "title": "CLIPLoader (GGUF)"
    }
  }
}